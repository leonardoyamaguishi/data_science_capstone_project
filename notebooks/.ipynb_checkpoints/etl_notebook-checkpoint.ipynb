{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29771f36",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Source: http://insideairbnb.com/get-the-data.html <br />\n",
    "Data from: Rio de Janeiro, Rio de Janeiro, Brazil <br />\n",
    "\n",
    "**Note:** <br />\n",
    "* The listings dataset wasn't considered in this analysis as the filesize would exceed Github's max filesize limit <br />\n",
    "* The reviews dataset had to be processed in order to fit the max filesize from Github\n",
    "<br />\n",
    "\n",
    "**Reviews_reduced creation script** \n",
    "<br />\n",
    "<code>reviews.drop(['id', 'reviewer_name', 'reviewer_id'], axis = 1, inplace = True)\n",
    "reviews.to_csv(data_filepath + 'reviews_reduced.csv')<code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9b2c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/leonardoyamaguishi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leonardoyamaguishi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leonardoyamaguishi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leonardoyamaguishi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Importing relevant liberaries for EDA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "# Modules for data processing\n",
    "import nltk\n",
    "\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f66a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haversine in /Users/leonardoyamaguishi/miniconda3/lib/python3.9/site-packages (2.5.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Library to calculate distance between two geo-locations\n",
    "# Source: https://towardsdatascience.com/calculating-distance-between-two-geolocations-in-python-26ad3afe287b\n",
    "!pip install haversine\n",
    "import haversine as hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d83538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/leonardoyamaguishi/Documents/Udacity/data_science_capstone_project/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath('')[:-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c7a6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonardoyamaguishi/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# Reading the datasets\n",
    "data_filepath = os.path.abspath('')[:-9] + '/data/'\n",
    "\n",
    "# calendar = pd.read_csv(data_filepath + 'calendar.csv') removed from the analysis, as the file is too heavy for Git\n",
    "listings = diet = pd.read_csv(data_filepath + 'listings.csv')\n",
    "\n",
    "reviews = pd.read_csv(data_filepath + 'reviews_reduced.csv')\n",
    "\n",
    "neighbourhood_hdi = pd.read_csv(data_filepath + 'neighbourhood_hdi.csv',\n",
    "                                sep = ';',\n",
    "                                decimal = ',',\n",
    "                                encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67025af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the geo-locations of metro statios in Rio de Janeiro\n",
    "# Data obatined from Google Maps\n",
    "# https://www.google.com/maps/search/estações+de+metro+rio+de+janeiros/@-22.8916528,-43.3403456,12z/data=!3m1!4b1\n",
    "# Collected in 24/01/2022\n",
    "\n",
    "metro_stations = {\n",
    "    'central': (-22.9049171010074, -43.19083837044949),\n",
    "    'gloria': (-22.920550514351582, -43.17648410807313),\n",
    "    'estacio': (-22.913496579807873, -43.2064926626525),\n",
    "    'botafogo': (-22.951082017707268, -43.18411168513209),\n",
    "    'pavuna': (-22.806666225646197, -43.36493532847317),\n",
    "    'cardeal_arcoverde' : (-22.96463512369287, -43.180656232771874),\n",
    "    'iraja' : (-22.84796020759417, -43.32343657311362),\n",
    "    'uruguaiana' : (-22.902940565469503, -43.18149436629887),\n",
    "    'cinelandia': (-22.911063930197916, -43.175678735438794),\n",
    "    'nova_america':(-22.879278506956446, -43.27187285300277),\n",
    "    'coelho_neto': (-22.83150705338417, -43.34329456463904),\n",
    "    'flamengo' : (-22.937231062820434, -43.17823192129637),\n",
    "    'cidade_nova' : (-22.908770113484287, -43.20630803329488),\n",
    "    'uruguai' : (-22.931835141573387, -43.23964451197971),\n",
    "    'vicente_carvalho' : (-22.85399039798257, -43.31315765809618),\n",
    "    'catete' : (-22.925787988052296, -43.17657983867064),\n",
    "    'acesso_a_cardeal_arcoverde' : (-22.964675568740653, -43.18054869617871),\n",
    "    'cantagalo' : (-22.976672769596224, -43.19385233439477),\n",
    "    'afonso_pena' : (-22.91829820485412, -43.21782218194045),\n",
    "    'general_osorio' : (-22.984766408179055, -43.19716911734364)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eea64211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the geo-locations of metro statios in Rio de Janeiro\n",
    "# Data obatined from Google Maps\n",
    "# https://www.google.com/maps/search/estações+de+metro+rio+de+janeiros/@-22.8916528,-43.3403456,12z/data=!3m1!4b1\n",
    "# Collected in 24/01/2022\n",
    "\n",
    "metro_stations_google_score = {\n",
    "    'central': 3.7,\n",
    "    'gloria': 4.4,\n",
    "    'estacio': 4.0,\n",
    "    'botafogo': 4.0,\n",
    "    'pavuna': 3.5,\n",
    "    'cardeal_arcoverde' : 4.3,\n",
    "    'iraja' : 3.8,\n",
    "    'uruguaiana' : 4.0,\n",
    "    'cinelandia': 4.4,\n",
    "    'nova_america': 4.2,\n",
    "    'coelho_neto': 3.7,\n",
    "    'flamengo' : 4.3,\n",
    "    'cidade_nova' : 4.2,\n",
    "    'uruguai' : 4.4,\n",
    "    'vicente_carvalho' : 4.1,\n",
    "    'catete' : 4.3,\n",
    "    'acesso_a_cardeal_arcoverde' : 4.3, # No score, but seems to be the same as cardeal_arcoverde\n",
    "    'cantagalo' : 4.4,\n",
    "    'afonso_pena' : 4.1,\n",
    "    'general_osorio': 4.1 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f1ac7",
   "metadata": {},
   "source": [
    "# ETL - Extract, transform, load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c6477",
   "metadata": {},
   "source": [
    "## Neighbourhood HDI ETL\n",
    "**Download source:** https://www.data.rio/documents/58186e41a2ad410f9099af99e46366fd/about <br />\n",
    "**Data source:** Dados básicos: IBGE-microdados dos Censos Demográficos 1991 e 2000.<br />\n",
    "**Note:** <br/>\n",
    "* To generate the csv file in the data folder, some formatting on Excel was performed translating the columns and removing columns not considered in this analysis.<br/>\n",
    "* IBGE - Brazilian Institute of Geography and Statistics.<br/>\n",
    "* Some neighbourhood names were adjusted in order to allow the retrieval of the HDI based on the neighbourhood names, some small research was done in order to increase the relieability of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484376fd",
   "metadata": {},
   "source": [
    "## Useful functions for neighbourhood_hdi ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f26f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_double_assessment(neighbourhood_hdi = neighbourhood_hdi):\n",
    "    '''\n",
    "    INPUT:\n",
    "    neighbourhood_hdi - (DataFrame) in which more than one neighbourhood was assessed at once\n",
    "    Note: In such cases, the different neighbourhoods were listed in the same row, separated by \",\"\n",
    "    \n",
    "    OUTPUT:\n",
    "    unpacked_neighbourhood_hdi - (DataFrame) with unpacked neighbourhoods (the rankings will be repeated)\n",
    "    '''\n",
    "    delist_after_treatment = []\n",
    "    \n",
    "    # Loops through indexes and neighbourhood names\n",
    "    for index, neighbourhood in enumerate(neighbourhood_hdi.neighbourhood):\n",
    "        \n",
    "        # If the neighbourhood column has ','\n",
    "        # means that more than one neighbourhood was assessed for the respective row\n",
    "        if neighbourhood_hdi['neighbourhood'][index].find(',') != -1:\n",
    "            \n",
    "            # Lists indexes to drop after unpacking\n",
    "            delist_after_treatment.append(index)\n",
    "            \n",
    "            # Lists the names inside the neighbourhood name column\n",
    "            names_to_unpack = neighbourhood_hdi.neighbourhood[index].split(',')\n",
    "            \n",
    "            # Copies the entire row as a template, as the only change in the unpacked values will be the name\n",
    "            template_series = neighbourhood_hdi.iloc[index]\n",
    "\n",
    "            # Loops through the names to unpack\n",
    "            for name in names_to_unpack:\n",
    "                # Duplicates the template series\n",
    "                series_to_append = template_series\n",
    "                # Changes the neighbourhood name from the template series\n",
    "                series_to_append['neighbourhood'] = name\n",
    "                # Appends the new unpacked neighbourhood\n",
    "                neighbourhood_hdi = neighbourhood_hdi.append(series_to_append, ignore_index = True)\n",
    "    \n",
    "    # After all iterations, the listed indexes to drop are dropped\n",
    "    unpacked_neighbourhood_hdi = neighbourhood_hdi.drop(delist_after_treatment, axis = 0)\n",
    "    \n",
    "    return unpacked_neighbourhood_hdi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "388ef197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_names(txt):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    txt - (string) in which the characters will be converted to lower case and the spaces will be removed\n",
    "    OUTPUTS:\n",
    "    standardized_txt - lower case (string) without spaces\n",
    "    '''\n",
    "    return str(txt.lower().replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1350300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbourhood_hdi_update_keys(neighbourhood_hdi = neighbourhood_hdi):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    neighbourhood_hdi - (DataFrame) to have the keys created\n",
    "    \n",
    "    OUTPUTS:\n",
    "    neighbourhood_hdi - (DataFrame) with updated keys according to their HDI rank\n",
    "    '''\n",
    "    neighbourhood_hdi.sort_values(by = 'human_development_index', ascending = False, inplace = True)\n",
    "    neighbourhood_hdi['neighbourhood_key'] = list(neighbourhood_hdi.index)\n",
    "    \n",
    "    return neighbourhood_hdi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812782e2",
   "metadata": {},
   "source": [
    "## neighbourhood_hdi ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f374e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hdi_ranking</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>per_capita_income_2000</th>\n",
       "      <th>longevity_index</th>\n",
       "      <th>education_index</th>\n",
       "      <th>income_index</th>\n",
       "      <th>human_development_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Gávea</td>\n",
       "      <td>2139.559275</td>\n",
       "      <td>0.924139</td>\n",
       "      <td>0.987167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Leblon</td>\n",
       "      <td>2441.279121</td>\n",
       "      <td>0.907799</td>\n",
       "      <td>0.993403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hdi_ranking neighbourhood  per_capita_income_2000  longevity_index  \\\n",
       "0            1         Gávea             2139.559275         0.924139   \n",
       "1            2        Leblon             2441.279121         0.907799   \n",
       "\n",
       "   education_index  income_index  human_development_index  \n",
       "0         0.987167           1.0                 0.970435  \n",
       "1         0.993403           1.0                 0.967067  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbourhood_hdi.head(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37537287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdi_ranking                                                   116\n",
      "neighbourhood              Camorim, Vargem Pequena, Vargem Grande\n",
      "per_capita_income_2000                                 279.093541\n",
      "longevity_index                                          0.688297\n",
      "education_index                                          0.836278\n",
      "income_index                                             0.712834\n",
      "human_development_index                                  0.745803\n",
      "Name: 115, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# In this IBGE study, some neighbourhoods were assessed together\n",
    "# In such cases the different neighbourhoods are listed in the same row, separated by \",\"\n",
    "print(neighbourhood_hdi.iloc[115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "236213c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbourhood column has only unique values: True\n"
     ]
    }
   ],
   "source": [
    "# Neighbourhood names unique values check\n",
    "neighbourhood_unnique = neighbourhood_hdi['neighbourhood'].nunique() == neighbourhood_hdi['neighbourhood'].shape[0]\n",
    "print('Neighbourhood column has only unique values: ' + str(neighbourhood_unnique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d63b73eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-a3cfc2665ea3>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series_to_append['neighbourhood'] = name\n"
     ]
    }
   ],
   "source": [
    "# Dropping the HDI Ranking as it can be redundant given the fact that the DataFrame can be sorted when necessary\n",
    "neighbourhood_hdi.drop('hdi_ranking', axis = 1, inplace = True)\n",
    "\n",
    "# Unpacking double assessments \n",
    "neighbourhood_hdi = unpack_double_assessment(neighbourhood_hdi)\n",
    "\n",
    "# Standardizing neighbourhood names\n",
    "neighbourhood_hdi['neighbourhood'] = neighbourhood_hdi['neighbourhood'].apply(standardize_names)\n",
    "\n",
    "# Updating neighbourhood keys\n",
    "neighbourhood_hdi = neighbourhood_hdi_update_keys(neighbourhood_hdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "344194b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>per_capita_income_2000</th>\n",
       "      <th>longevity_index</th>\n",
       "      <th>education_index</th>\n",
       "      <th>income_index</th>\n",
       "      <th>human_development_index</th>\n",
       "      <th>neighbourhood_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gávea</td>\n",
       "      <td>2139.559275</td>\n",
       "      <td>0.924139</td>\n",
       "      <td>0.987167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.970435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>leblon</td>\n",
       "      <td>2441.279121</td>\n",
       "      <td>0.907799</td>\n",
       "      <td>0.993403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  neighbourhood  per_capita_income_2000  longevity_index  education_index  \\\n",
       "0         gávea             2139.559275         0.924139         0.987167   \n",
       "1        leblon             2441.279121         0.907799         0.993403   \n",
       "\n",
       "   income_index  human_development_index  neighbourhood_key  \n",
       "0           1.0                 0.970435                  0  \n",
       "1           1.0                 0.967067                  1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbourhood_hdi.head(n = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404f4ee",
   "metadata": {},
   "source": [
    "## Useful functions for Listings ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5554eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    text - a string to be tokenized\n",
    "    OUTPUTS:\n",
    "    clean_tokens - (list) of clean tokens\n",
    "    '''\n",
    "    \n",
    "    # Removing stop words \n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenizing the text\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "    # Removing stopwords - assuming that all amenities are listed in english\n",
    "    tokenized_text = [w for w in tokenized_text if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # If the text is not a stop word, it should be tokenized\n",
    "    if tokenized_text != []:\n",
    "        # Starts the lemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Creates a list of lemmatized tokens\n",
    "        clean_tok = lemmatizer.lemmatize(tokenized_text[0]).lower().strip()\n",
    "    else:\n",
    "        clean_tok = ''\n",
    "        \n",
    "    return clean_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccac4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_list_to_string(token_list):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    token_list - (list) of tokens\n",
    "    \n",
    "    OUTPUTS:\n",
    "    token_list_string - (str) in which the tokens are separated by ' '\n",
    "    '''\n",
    "    token_list_string = ''\n",
    "    \n",
    "    n = 0\n",
    "    \n",
    "    for n in range (0, len(token_list)):\n",
    "        \n",
    "        # On the start of the loop, no space is necessary\n",
    "        if n != 0:\n",
    "            token_list_string += ' '\n",
    "            \n",
    "        # adds on the token_list_string the a unique token (can be a compound noun)\n",
    "        token_list_string += token_list[n]\n",
    "        \n",
    "    return token_list_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea5f483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def literal_to_list_and_tokenize(list_string):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    listings_series - (Series) with a column with literal list strings composed of strings to be tokenized\n",
    "    \n",
    "    OUTPUTS:\n",
    "    token_list_as_string - (str) in which the tokens are separated by ' ' (adapted for CountVectorizer)\n",
    "    '''\n",
    "    \n",
    "    # Converts literal list to list\n",
    "    converted_list = ast.literal_eval(list_string)\n",
    "          \n",
    "    for i in range (0, len(converted_list)):\n",
    "        \n",
    "        # As each item in the list is a item that could be a compound noun\n",
    "        # The objective is to tokenize each element of the compound noun and join then with ' '\n",
    "        # Increasing the chance of finding matches throughout the dataset\n",
    "        \n",
    "        compound_noun = converted_list[i].split()\n",
    "        \n",
    "        generated_word = ''\n",
    "        \n",
    "        for l in range(0, len(compound_noun)):\n",
    "            tokenized_element = tokenize(str(compound_noun[l]).lower())\n",
    "            \n",
    "            # Tokenized_elemnt is '' when the given word is a stop word\n",
    "            if tokenized_element != '':\n",
    "                \n",
    "                # If not a stop word, check if it's the first element of the possible compound noun\n",
    "                if l != 0:\n",
    "                    generated_word += ' '\n",
    "                    \n",
    "                # If compound_noun has length above 1, it's a compound noun\n",
    "                generated_word += tokenize(str(compound_noun[l]).lower())\n",
    "        \n",
    "        # Adds the compound noun to the converted list\n",
    "        converted_list[i] = generated_word\n",
    "\n",
    "    # Convert the converted_list of tokenized compound nouns to a string separating them with ' '\n",
    "    token_list_as_string = token_list_to_string(converted_list)\n",
    "    \n",
    "    return token_list_as_string #ASSESSMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "369fa8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_nan_columns(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - (DataFrame) in which the columns composed entirely of NaN values will be listed\n",
    "    OUTPUT:\n",
    "    nan_columns_list - (list) of columns composed entirely of NaN values\n",
    "    '''\n",
    "    \n",
    "    nan_columns_list = []\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if df[column].isna().sum() == df.shape[0]:\n",
    "            nan_columns_list.append(column)\n",
    "            \n",
    "    return nan_columns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65ddab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walking_dist_stations(listings = listings, metro_stations = metro_stations, walking_dist = 1):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    listings - (DataFrame) with AirBnb listings with latitude and longitude columns\n",
    "    metro_stations - (Dictionary) with the name of the metro stations as keys and (tuples)\n",
    "    of their respective latitude and longitude\n",
    "    \n",
    "    OUTPUTS:\n",
    "    nearest_stations_walking_dist - (list) of the nearest stations within a distance of 1 km\n",
    "    '''\n",
    "    \n",
    "    # Prepares the lists\n",
    "    nearest_stations_walking_dist = []\n",
    "    \n",
    "    # Loops through the latitudes and longitudes of the AirBnBs\n",
    "    for latitude, longitude in zip(listings.latitude, listings.longitude):\n",
    "        \n",
    "        # Distance place holder\n",
    "        nearest_station = 'NONE'\n",
    "        nearest_station_dist = 999999\n",
    "        \n",
    "        # Loops through the metro_stations dictionary\n",
    "        for station in metro_stations:\n",
    "            \n",
    "            # Computes distance according to the Haversine distance\n",
    "            dist = hs.haversine((latitude, longitude), metro_stations[station])\n",
    "            \n",
    "            # Updates the variables if a nearer metro station is found\n",
    "            if dist < nearest_station_dist:\n",
    "                nearest_station = station\n",
    "                nearest_station_dist = dist\n",
    "        \n",
    "        # Updates the lists\n",
    "        if dist <= walking_dist:\n",
    "            nearest_stations_walking_dist.append(station)\n",
    "        else:\n",
    "            nearest_stations_walking_dist.append('NONE')\n",
    "        \n",
    "        station_score_list = assign_station_score(nearest_stations_walking_dist, metro_stations_google_score)\n",
    "    return nearest_stations_walking_dist, station_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f57c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listings_assign_keys(listings = listings, neighbourhood_hdi = neighbourhood_hdi):\n",
    "    '''\n",
    "    INPUTS: \n",
    "    listings - (DataFrame) with 'neighbourhood' column that matches the neighbourhoods in neighbourhood_hdi\n",
    "    neighbourhood_hdi - (DataFrame) with 'neighbourhood' and 'neighbourhood_key' columns\n",
    "    \n",
    "    OUTPUTS:\n",
    "    listings - (DataFrame) with new or updated 'neighrbouhood_key' column matching neighbourhood_hdi\n",
    "    '''\n",
    "    \n",
    "    # Updating neighbourhood_hdi keys\n",
    "    neighbourhood_hdi = neighbourhood_hdi_update_keys(neighbourhood_hdi)\n",
    "    \n",
    "    # Creates a dictionary that connects the neighbourhood name to its respective key\n",
    "    name_key_dict = {}\n",
    "\n",
    "    for name, key in zip(neighbourhood_hdi.neighbourhood, neighbourhood_hdi.neighbourhood_key):\n",
    "        name_key_dict[name] = key\n",
    "\n",
    "    listings_keys = []\n",
    "\n",
    "    # Assigns the secondary key to listings 'neighbourhood_key' column according to its 'neighbourhood'\n",
    "    for name in listings.neighbourhood:\n",
    "        listings_keys.append(name_key_dict[name])\n",
    "\n",
    "    listings['neighbourhood_key'] = listings_keys\n",
    "    \n",
    "    return listings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67133df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_station_score(stations_list, metro_stations_google_score = metro_stations_google_score):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    stations_list - (list) of metro stations mapped in the dictionaries declared in the start of the notebook\n",
    "    metro_stations_google_score - (dictionary) that links the metro stations (keys) with their respective\n",
    "    google scores\n",
    "    \n",
    "    OUTPUTS:\n",
    "    station_score_list - (list) of google scores in the same order as the stations_list\n",
    "\n",
    "    '''\n",
    "    \n",
    "    station_score_list = []\n",
    "    \n",
    "    for station in stations_list:\n",
    "        if station != \"NONE\":\n",
    "            station_score_list.append(metro_stations_google_score[station])\n",
    "        else: \n",
    "            # -1 is assigned to the case NONE\n",
    "            station_score_list.append(-1)\n",
    "            \n",
    "    return station_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "252c2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_bathroom_string(listings = listings):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    listings - DataFrame with bathrooms_text, in which \"shared\" indicated shared bathrooms\n",
    "    \n",
    "    OUTPUTS:\n",
    "    shared_bathroom - (list) with 1 for True and 0 for False\n",
    "    bathroom_qty - (list) quantity of bathroom for the respective listing\n",
    "    '''\n",
    "    shared_bathroom = []\n",
    "    bathroom_qty = []\n",
    "\n",
    "    for txt in listings.bathrooms_text:\n",
    "        \n",
    "        # Retrieving the bathroom quantity\n",
    "        try:\n",
    "            bathroom_qty.append(float(str(txt)[0:(str(txt).lower().find(' '))]))\n",
    "        except:\n",
    "            # Some strings do not have quantity, therefore are considered as nan\n",
    "            bathroom_qty.append(np.nan)\n",
    "            \n",
    "        # If the bathrooms_text has shared in the string, it indicates a shared bathroom\n",
    "        if str(txt).lower().find('shared') != -1:\n",
    "            shared_bathroom.append(1)\n",
    "        else:\n",
    "            shared_bathroom.append(0)\n",
    "            \n",
    "    return bathroom_qty, shared_bathroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7cd5574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_boolean_columns(txt):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    txt - (string or nan) with \"t\" for True and \"f\" for False\n",
    "    OUTPUTS:\n",
    "    1 - (int) for \"t\"\n",
    "    0 - (int) for \"f\" and nan\n",
    "    '''\n",
    "    if txt == 't': # Case t (True)\n",
    "        return 1\n",
    "    else: # Cases f (False) and nan\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618a4ed",
   "metadata": {},
   "source": [
    "## Listings ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abbda6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>listing_url</th>\n",
       "      <th>scrape_id</th>\n",
       "      <th>last_scraped</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>neighborhood_overview</th>\n",
       "      <th>picture_url</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_url</th>\n",
       "      <th>...</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>license</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>calculated_host_listings_count_entire_homes</th>\n",
       "      <th>calculated_host_listings_count_private_rooms</th>\n",
       "      <th>calculated_host_listings_count_shared_rooms</th>\n",
       "      <th>reviews_per_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17878</td>\n",
       "      <td>https://www.airbnb.com/rooms/17878</td>\n",
       "      <td>20211224070558</td>\n",
       "      <td>2021-12-25</td>\n",
       "      <td>Very Nice 2Br in Copacabana w. balcony, fast WiFi</td>\n",
       "      <td>Discounts for long term stays. &lt;br /&gt;- Large b...</td>\n",
       "      <td>This is the one of the bests spots in Rio. Bec...</td>\n",
       "      <td>https://a0.muscache.com/pictures/65320518/3069...</td>\n",
       "      <td>68997</td>\n",
       "      <td>https://www.airbnb.com/users/show/68997</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.66</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24480</td>\n",
       "      <td>https://www.airbnb.com/rooms/24480</td>\n",
       "      <td>20211224070558</td>\n",
       "      <td>2021-12-25</td>\n",
       "      <td>Nice and cozy near Ipanema Beach, w/ home office</td>\n",
       "      <td>My studio is located in the best of Ipanema, t...</td>\n",
       "      <td>The beach, the lagoon, Ipanema is a great loca...</td>\n",
       "      <td>https://a0.muscache.com/pictures/11955612/b28e...</td>\n",
       "      <td>99249</td>\n",
       "      <td>https://www.airbnb.com/users/show/99249</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.97</td>\n",
       "      <td>4.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                         listing_url       scrape_id last_scraped  \\\n",
       "0  17878  https://www.airbnb.com/rooms/17878  20211224070558   2021-12-25   \n",
       "1  24480  https://www.airbnb.com/rooms/24480  20211224070558   2021-12-25   \n",
       "\n",
       "                                                name  \\\n",
       "0  Very Nice 2Br in Copacabana w. balcony, fast WiFi   \n",
       "1   Nice and cozy near Ipanema Beach, w/ home office   \n",
       "\n",
       "                                         description  \\\n",
       "0  Discounts for long term stays. <br />- Large b...   \n",
       "1  My studio is located in the best of Ipanema, t...   \n",
       "\n",
       "                               neighborhood_overview  \\\n",
       "0  This is the one of the bests spots in Rio. Bec...   \n",
       "1  The beach, the lagoon, Ipanema is a great loca...   \n",
       "\n",
       "                                         picture_url  host_id  \\\n",
       "0  https://a0.muscache.com/pictures/65320518/3069...    68997   \n",
       "1  https://a0.muscache.com/pictures/11955612/b28e...    99249   \n",
       "\n",
       "                                  host_url  ... review_scores_communication  \\\n",
       "0  https://www.airbnb.com/users/show/68997  ...                         4.9   \n",
       "1  https://www.airbnb.com/users/show/99249  ...                         4.9   \n",
       "\n",
       "  review_scores_location review_scores_value license instant_bookable  \\\n",
       "0                   4.76                4.66     NaN                f   \n",
       "1                   4.97                4.58     NaN                f   \n",
       "\n",
       "  calculated_host_listings_count calculated_host_listings_count_entire_homes  \\\n",
       "0                              1                                           1   \n",
       "1                              1                                           1   \n",
       "\n",
       "  calculated_host_listings_count_private_rooms  \\\n",
       "0                                            0   \n",
       "1                                            0   \n",
       "\n",
       "  calculated_host_listings_count_shared_rooms reviews_per_month  \n",
       "0                                           0              1.92  \n",
       "1                                           0              0.62  \n",
       "\n",
       "[2 rows x 74 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings.head(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce06301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "delisted_columns = [\n",
    "                            'name',\n",
    "                            'description',\n",
    "                            'neighborhood_overview',\n",
    "                            'listing_url',\n",
    "                            'scrape_id',\n",
    "                            'last_scraped',\n",
    "                            'picture_url',\n",
    "                            'host_picture_url',\n",
    "                            'host_url',\n",
    "                            'host_location',\n",
    "                            'host_thumbnail_url',\n",
    "                            'host_total_listings_count',\n",
    "                            'host_name',\n",
    "                            'host_about',\n",
    "                            'host_neighbourhood',\n",
    "                            'host_verifications',\n",
    "                            'property_type', \n",
    "                            'neighbourhood',\n",
    "                            'availability_30',\n",
    "                            'availability_60',\n",
    "                            'availability_90',\n",
    "                            'availability_365',\n",
    "                            'instant_bookable',\n",
    "                            'calculated_host_listings_count',\n",
    "                            'calculated_host_listings_count_entire_homes',\n",
    "                            'calculated_host_listings_count_private_rooms',\n",
    "                            'calculated_host_listings_count_shared_rooms',\n",
    "                            'minimum_nights',\n",
    "                            'maximum_nights',\n",
    "                            'minimum_minimum_nights',\n",
    "                            'maximum_minimum_nights',\n",
    "                            'minimum_maximum_nights',\n",
    "                            'maximum_maximum_nights',\n",
    "                            'has_availability',\n",
    "                            'calendar_last_scraped',\n",
    "                            'number_of_reviews',\n",
    "                            'first_review',\n",
    "                            'last_review',\n",
    "                            'host_acceptance_rate',\n",
    "                            'host_listings_count',\n",
    "                            'number_of_reviews_l30d'\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d35a0ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hypothesis: listings.id is a column of primary key values\n",
    "listings.id.nunique() == listings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d32a7bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "[(nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan), (nan, nan)]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating possible redundant column\n",
    "print(str((listings.host_listings_count == listings.host_total_listings_count).sum() == listings.shape[0]) + '\\n')\n",
    "\n",
    "diff_list = []\n",
    "for control, test in zip(listings.host_listings_count, listings.host_total_listings_count):\n",
    "    if control != test:\n",
    "        diff_list.append((control, test))\n",
    "        \n",
    "print(diff_list)\n",
    "\n",
    "# Conclusion: any of the 2 evaluated columnns can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "044246cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking and dropping columns composed entirely of NaN values\n",
    "listings_columns_to_drop = list_nan_columns(listings)\n",
    "listings.drop(listings_columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "# Dropping delisted columns, these columns were dropped as they were out of the scope of the analysis\n",
    "# given their definition\n",
    "listings.drop(delisted_columns, axis = 1, inplace = True)\n",
    "\n",
    "# Summarizing and replacing the bathrooms_text column\n",
    "bathroom_qty, shared_bathroom = summarize_bathroom_string(listings)\n",
    "listings['bathroom_qty'] = bathroom_qty\n",
    "listings['shared_bathroom'] = shared_bathroom\n",
    "listings.drop(['bathrooms_text'], axis = 1, inplace = True)\n",
    "\n",
    "# Standardizing boolean columns\n",
    "listings['host_has_profile_pic'] = listings['host_has_profile_pic'].apply(standardize_boolean_columns)\n",
    "listings['host_identity_verified'] = listings['host_identity_verified'].apply(standardize_boolean_columns)\n",
    "listings['host_is_superhost'] = listings['host_is_superhost'].apply(standardize_boolean_columns)\n",
    "\n",
    "# Converting literal lists to token strings\n",
    "listings['amenities'] = listings['amenities'].apply(literal_to_list_and_tokenize)\n",
    "\n",
    "# Computing the nearest stations\n",
    "walking_dist_stations, walking_dist_stations_scores = compute_walking_dist_stations(listings, metro_stations)\n",
    "\n",
    "# Appending in the DataFrame\n",
    "listings['walking_dist_stations'] = walking_dist_stations\n",
    "listings['walking_dist_stations_scores'] = walking_dist_stations_scores\n",
    "\n",
    "# Dropping the latitude and longitude columns\n",
    "listings.drop(['latitude', 'longitude'], axis = 1, inplace = True)\n",
    "\n",
    "# Standardizing neighbourhood names\n",
    "listings['neighbourhood'] = listings.neighbourhood_cleansed.apply(standardize_names)\n",
    "\n",
    "# Dropping the non standardized column\n",
    "listings.drop(['neighbourhood_cleansed'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "539eb85b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gericinó'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assessing the neighbourhoods that could not be retrieved from the IBGE data\n",
    "\n",
    "assess = []\n",
    "for neighbourhood in listings.neighbourhood:\n",
    "    if neighbourhood not in list(neighbourhood_hdi['neighbourhood']):\n",
    "        assess.append(neighbourhood)\n",
    "\n",
    "set(assess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "237cfcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'gericinó'the missing data will be the average of the hdi table\n",
    "\n",
    "# Creating a series with the neighbourhood data\n",
    "gericino_case = pd.Series(data = 'gericinó', index = ['neighbourhood'])\n",
    "\n",
    "# Calculating the mean for the numeric columns\n",
    "gericino_series = neighbourhood_hdi.mean().append(gericino_case)\n",
    "\n",
    "# Reindexing the series to append in neighbourhood_hdi\n",
    "gericino_series.reindex(['neighbourhood',\n",
    "                        'per_capita_income_2000',\n",
    "                        'longevity_index', \n",
    "                        'education_index', \n",
    "                        'income_index', \n",
    "                        'human_development_index'])\n",
    "\n",
    "# Appending new data \n",
    "neighbourhood_hdi = neighbourhood_hdi.append(gericino_series, ignore_index = True)\n",
    "\n",
    "# Updating neighbourhood_hdi keys\n",
    "neighbourhood_hdi = neighbourhood_hdi_update_keys(neighbourhood_hdi)\n",
    "\n",
    "# Assigning keys to the neighbourhoods in listings.neighbourhoods\n",
    "listings = listings_assign_keys(listings , neighbourhood_hdi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f67ba1ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Expading the listings DataFrame\n",
    "listings_expanded = listings.join(neighbourhood_hdi, on = 'neighbourhood_key', how = 'left', lsuffix = '_hdi')\n",
    "\n",
    "# Dropping the key columns and neighbourhood names\n",
    "listings_expanded.drop(['neighbourhood_key',\n",
    "                        'neighbourhood_key_hdi',\n",
    "                        'neighbourhood_hdi',\n",
    "                        'neighbourhood']\n",
    "                       , axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b01b8468",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(tokenizer=None)\n",
    "\n",
    "# Getting the counts for each token\n",
    "X = vect.fit_transform(listings_expanded['amenities'])\n",
    "\n",
    "# Converting to dense matrix to save\n",
    "X_dense = scipy.sparse.csr_matrix.todense(X)\n",
    "\n",
    "# Inverting the dictionary from {key : value} to {value : key}\n",
    "# Source: https://stackoverflow.com/questions/483666/reverse-invert-a-dictionary-mapping\n",
    "matrix_vocabulary = {value: key for key, value in vect.vocabulary_.items()}\n",
    "\n",
    "# Retrieving the columns from the CountVectorizer matrix according to the generated vocabulary\n",
    "matrix_columns = []\n",
    "for i in range(0, len(matrix_vocabulary)):\n",
    "    matrix_columns.append('amn_' + matrix_vocabulary[i])\n",
    "    \n",
    "# Converting the dense matrix to DataFrame\n",
    "amenities_dataframe = pd.DataFrame(X_dense, columns = matrix_columns, index = listings_expanded.index)\n",
    "\n",
    "# Dropping amenities from listings_expanded\n",
    "listings_expanded.drop(['amenities'], axis = 1, inplace = True)\n",
    "\n",
    "# Saving the processed DataFrames as csv\n",
    "amenities_dataframe.to_csv(data_filepath + 'amenities_processed.csv')\n",
    "listings_expanded.to_csv(data_filepath + 'listings_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd38b67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_since</th>\n",
       "      <th>host_response_time</th>\n",
       "      <th>host_response_rate</th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>room_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>...</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>bathroom_qty</th>\n",
       "      <th>shared_bathroom</th>\n",
       "      <th>walking_dist_stations</th>\n",
       "      <th>walking_dist_stations_scores</th>\n",
       "      <th>per_capita_income_2000</th>\n",
       "      <th>longevity_index</th>\n",
       "      <th>education_index</th>\n",
       "      <th>income_index</th>\n",
       "      <th>human_development_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17878</td>\n",
       "      <td>68997</td>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>within an hour</td>\n",
       "      <td>100%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NONE</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1623.415521</td>\n",
       "      <td>0.879608</td>\n",
       "      <td>0.989888</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24480</td>\n",
       "      <td>99249</td>\n",
       "      <td>2010-03-26</td>\n",
       "      <td>a few days or more</td>\n",
       "      <td>100%</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>general_osorio</td>\n",
       "      <td>4.1</td>\n",
       "      <td>2465.445144</td>\n",
       "      <td>0.894594</td>\n",
       "      <td>0.991860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  host_id  host_since  host_response_time host_response_rate  \\\n",
       "0  17878    68997  2010-01-08      within an hour               100%   \n",
       "1  24480    99249  2010-03-26  a few days or more               100%   \n",
       "\n",
       "   host_is_superhost  host_has_profile_pic  host_identity_verified  \\\n",
       "0                  0                     1                       1   \n",
       "1                  0                     1                       1   \n",
       "\n",
       "         room_type  accommodates  ...  reviews_per_month  bathroom_qty  \\\n",
       "0  Entire home/apt             5  ...               1.92           1.0   \n",
       "1  Entire home/apt             2  ...               0.62           1.0   \n",
       "\n",
       "  shared_bathroom  walking_dist_stations  walking_dist_stations_scores  \\\n",
       "0               0                   NONE                          -1.0   \n",
       "1               0         general_osorio                           4.1   \n",
       "\n",
       "   per_capita_income_2000  longevity_index  education_index  income_index  \\\n",
       "0             1623.415521         0.879608         0.989888           1.0   \n",
       "1             2465.445144         0.894594         0.991860           1.0   \n",
       "\n",
       "   human_development_index  \n",
       "0                 0.956499  \n",
       "1                 0.962151  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_expanded.head(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "020143ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amn_02</th>\n",
       "      <th>amn_04</th>\n",
       "      <th>amn_10</th>\n",
       "      <th>amn_100</th>\n",
       "      <th>amn_101</th>\n",
       "      <th>amn_102</th>\n",
       "      <th>amn_104</th>\n",
       "      <th>amn_105</th>\n",
       "      <th>amn_107</th>\n",
       "      <th>amn_10g</th>\n",
       "      <th>...</th>\n",
       "      <th>amn_xampu</th>\n",
       "      <th>amn_xbox</th>\n",
       "      <th>amn_xiaomi</th>\n",
       "      <th>amn_xiomi</th>\n",
       "      <th>amn_xxx</th>\n",
       "      <th>amn_yamaha</th>\n",
       "      <th>amn_yamasterol</th>\n",
       "      <th>amn_year</th>\n",
       "      <th>amn_youtube</th>\n",
       "      <th>amn_yp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amn_02  amn_04  amn_10  amn_100  amn_101  amn_102  amn_104  amn_105  \\\n",
       "0       0       0       0        0        0        0        0        0   \n",
       "1       0       0       0        0        0        0        0        0   \n",
       "\n",
       "   amn_107  amn_10g  ...  amn_xampu  amn_xbox  amn_xiaomi  amn_xiomi  amn_xxx  \\\n",
       "0        0        0  ...          0         0           0          0        0   \n",
       "1        0        0  ...          0         0           0          0        0   \n",
       "\n",
       "   amn_yamaha  amn_yamasterol  amn_year  amn_youtube  amn_yp  \n",
       "0           0               0         0            0       0  \n",
       "1           0               0         0            0       0  \n",
       "\n",
       "[2 rows x 1112 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amenities_dataframe.head(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f62ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
